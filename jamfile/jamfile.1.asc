JAMFILE(1)                   General Commands Manual                  JAMFILE(1)

NNAAMMEE
     jjaammffiillee - large language model quantizer

SSYYNNOOPPSSIISS
     jjaammffiillee [flags...] _m_o_d_e_l_-_f_3_2_._g_g_u_f [_m_o_d_e_l_-_q_u_a_n_t_._g_g_u_f] _t_y_p_e [_n_t_h_r_e_a_d_s]

DDEESSCCRRIIPPTTIIOONN
     jjaammffiillee converts large language model weights from the float32 or float16
     formats into smaller data types from 2 to 8 bits in size.

OOPPTTIIOONNSS
     The following flags are available:

     ----aallllooww--rreeqquuaannttiizzee
             Allows requantizing tensors that have already been quantized.
             Warning: This can severely reduce quality compared to quantizing
             from 16bit or 32bit

     ----lleeaavvee--oouuttppuutt--tteennssoorr
             Will leave output.weight un(re)quantized. Increases model size but
             may also increase quality, especially when requantizing

     ----ppuurree  Disable k-quant mixtures and quantize all tensors to the same type

AARRGGUUMMEENNTTSS
     The following positional arguments are accepted:

     _m_o_d_e_l_-_f_3_2_._g_g_u_f
             Is the input file, which contains the unquantized model weights in
             either the float32 or float16 format.

     _m_o_d_e_l_-_q_u_a_n_t_._g_g_u_f
             Is the output file, which will contain quantized weights in the
             desired format. If this path isn't specified, it'll default to [inp
             path]/ggml-model-[ftype].gguf.

     _t_y_p_e    Is the desired quantization format, which may be the integer id of
             a supported quantization type, or its name. See the quantization
             types section below for acceptable formats.

     _n_t_h_r_e_a_d_s
             Number of threads to use during computation (default: nproc/2)

QQUUAANNTTIIZZAATTIIOONN TTYYPPEESS
     The following quantization types are available. This table shows the ID of
     the quantization format, its name, the file size of 7B model weights that
     use it, and finally the amount of quality badness it introduces as measured
     by the llamafile-perplexity tool averaged over 128 chunks with the
     TinyLLaMA 1.1B v1.0 Chat model. Rows are ordered in accordance with how
     recommended the quantization format is for general usage.

     --     18 Q6_K   5.6gb +0.0446 ppl (q6 kawrakow)
     --      7 Q8_0   7.2gb +0.0022 ppl (q8 gerganov)
     --      1 F16    14gb  +0.0000 ppl (best but biggest)
     --      8 Q5_0   4.7gb +0.0817 ppl (q5 gerganov zero)
     --     17 Q5_K_M 4.8gb +0.0836 ppl (q5 kawrakow medium)
     --     16 Q5_K_S 4.7gb +0.1049 ppl (q5 kawrakow small)
     --     15 Q4_K_M 4.1gb +0.3132 ppl (q4 kawrakow medium)
     --     14 Q4_K_S 3.9gb +0.3408 ppl (q4 kawrakow small)
     --     13 Q3_K_L 3.6gb +0.5736 ppl (q3 kawrakow large)
     --     12 Q3_K_M 3.3gb +0.7612 ppl (q3 kawrakow medium)
     --     11 Q3_K_S 3.0gb +1.3834 ppl (q3 kawrakow small)
     --     10 Q2_K   2.6gb +4.2359 ppl (tiniest hallucinates most)
     --     32 BF16   14gb  +0.0000 ppl (canonical but cpu/cuda only)
     --      0 F32    27gb   9.0952 ppl (reference point)
     --      2 Q4_0   3.9gb +0.3339 ppl (legacy)
     --      3 Q4_1   4.3gb +0.4163 ppl (legacy)
     --      9 Q5_1   5.1gb +0.1091 ppl (legacy)
     --     12 Q3_K   alias for Q3_K_M
     --     15 Q4_K   alias for Q4_K_M
     --     17 Q5_K   alias for Q5_K_M
     --   COPY Only copy tensors, no quantizing.

SSEEEE AALLSSOO
     llamafile(1), llamafile-imatrix(1), llamafile-perplexity(1),
     llava-quantize(1), zipalign(1), unzip(1)

Llamafile Manual                December 5, 2023                Llamafile Manual
