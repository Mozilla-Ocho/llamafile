[4mLLAMAFILE[24m(1)                General Commands Manual               [4mLLAMAFILE[24m(1)

[1mNAME[0m
       llamafile ‚Äî large language model runner

[1mSYNOPSIS[0m
       [1mllamafile [22m[[1m--server[22m] [flags...] [1m-m [4m[22mmodel.gguf[24m [[1m--mmproj [4m[22mvision.gguf[24m]
       [1mllamafile [22m[[1m--cli[22m] [flags...] [1m-m [4m[22mmodel.gguf[24m [1m-p [4m[22mprompt[0m
       [1mllamafile [22m[[1m--cli[22m] [flags...] [1m-m [4m[22mmodel.gguf[24m [1m--mmproj [4m[22mvision.gguf[24m [1m--image[0m
                 [4mgraphic.png[24m [1m-p [4m[22mprompt[0m

[1mDESCRIPTION[0m
       [1mllamafile [22mis a large language model tool. It has use cases such as:

       [1m-   [22mCode completion
       [1m-   [22mProse composition
       [1m-   [22mChatbot that passes the Turing test
       [1m-   [22mText/image summarization and analysis

[1mOPTIONS[0m
       The following options are available:

       [1m--version[0m
               Print version and exit.

       [1m-h[22m, [1m--help[0m
               Show help message and exit.

       [1m--cli   [22mPuts  program  in command line interface mode. This flag is im‚Äê
               plied when a prompt is supplied  using  either  the  [1m-p  [22mor  [1m-f[0m
               flags.

       [1m--server[0m
               Puts program in server mode. This will launch an HTTP server on
               a  local  port. This server has both a web UI and an OpenAI API
               compatible completions endpoint. When the server is  run  on  a
               desk  system,  a tab browser tab will be launched automatically
               that displays the web UI.  This [1m--server [22mflag is implied if  no
               prompt  is  specified,  i.e.  neither  the  [1m-p  [22mor [1m-f [22mflags are
               passed.

       [1m-m [4m[22mFNAME[24m, [1m--model [4m[22mFNAME[0m
               Model path in the GGUF file format.

               Default: [4mmodels/7B/ggml‚Äêmodel‚Äêf16.gguf[0m

       [1m--mmproj [4m[22mFNAME[0m
               Specifies path of the LLaVA vision model in the GGUF file  for‚Äê
               mat.  If  this  flag  is supplied, then the [1m--model [22mand [1m--image[0m
               flags should also be supplied.

       [1m-s [4m[22mSEED[24m, [1m--seed [4m[22mSEED[0m
               Random Number Generator (RNG) seed. A random seed  is  used  if
               this is less than zero.

               Default: ‚Äê1

       [1m-t [4m[22mN[24m, [1m--threads [4m[22mN[0m
               Number of threads to use during generation.

               Default: $(nproc)/2

       [1m-tb [4m[22mN[24m, [1m--threads‚Äêbatch [4m[22mN[0m
               Set  the  number of threads to use during batch and prompt pro‚Äê
               cessing. In some systems, it is beneficial to use a higher num‚Äê
               ber of threads during batch processing than during  generation.
               If not specified, the number of threads used for batch process‚Äê
               ing  will be the same as the number of threads used for genera‚Äê
               tion.

               Default: Same as [1m--threads[0m

       [1m-td [4m[22mN[24m, [1m--threads‚Äêdraft [4m[22mN[0m
               Number of threads to use during generation.

               Default: Same as [1m--threads[0m

       [1m-tbd [4m[22mN[24m, [1m--threads‚Äêbatch‚Äêdraft [4m[22mN[0m
               Number of threads to use during batch and prompt processing.

               Default: Same as [1m--threads‚Äêdraft[0m

       [1m--in‚Äêprefix‚Äêbos[0m
               Prefix BOS to user inputs, preceding the [1m--in‚Äêprefix [22mstring.

       [1m--in‚Äêprefix [4m[22mSTRING[0m
               This flag is used to add a prefix  to  your  input,  primarily,
               this is used to insert a space after the reverse prompt. Here‚Äôs
               an  example  of  how to use the [1m--in‚Äêprefix [22mflag in conjunction
               with the [1m--reverse‚Äêprompt [22mflag:

                     [1m./main ‚Äêr "User:" ‚Äê‚Äêin‚Äêprefix " "[0m

               Default: empty

       [1m--in‚Äêsuffix [4m[22mSTRING[0m
               This flag is used to add a suffix after  your  input.  This  is
               useful  for  adding an "Assistant:" prompt after the user‚Äôs in‚Äê
               put. It‚Äôs added after the new‚Äêline character (\n) that‚Äôs  auto‚Äê
               matically added to the end of the user‚Äôs input. Here‚Äôs an exam‚Äê
               ple  of how to use the [1m--in‚Äêsuffix [22mflag in conjunction with the
               [1m--reverse‚Äêprompt [22mflag:

                     [1m./main   ‚Äêr   "User:"   ‚Äê‚Äêin‚Äêprefix   "   "   ‚Äê‚Äêin‚Äêsuffix[0m
                     [1m"Assistant:"[0m

               Default: empty

       [1m-n [4m[22mN[24m, [1m--n‚Äêpredict [4m[22mN[0m
               Number of tokens to predict.

               [1m-   [22m‚Äê1 = infinity
               [1m-   [22m‚Äê2 = until context filled

               Default: ‚Äê1

       [1m-c [4m[22mN[24m, [1m--ctx‚Äêsize [4m[22mN[0m
               Set the size of the prompt context. A larger context size helps
               the  model  to  better  comprehend  and  generate responses for
               longer input or conversations. The LLaMA models were built with
               a context of 2048, which yields the best results on longer  in‚Äê
               put / inference.

               [1m-   [22m0 = loaded automatically from model

               Default: 512

       [1m-b [4m[22mN[24m, [1m--batch‚Äêsize [4m[22mN[0m
               Batch size for prompt processing.

               Default: 512

       [1m--top‚Äêk [4m[22mN[0m
               Top‚Äêk sampling.

               [1m-   [22m0 = disabled

               Default: 40

       [1m--top‚Äêp [4m[22mN[0m
               Top‚Äêp sampling.

               [1m-   [22m1.0 = disabled

               Default: 0.9

       [1m--min‚Äêp [4m[22mN[0m
               Min‚Äêp sampling.

               [1m-   [22m0.0 = disabled

               Default: 0.1

       [1m--tfs [4m[22mN[0m
               Tail free sampling, parameter z.

               [1m-   [22m1.0 = disabled

               Default: 1.0

       [1m--typical [4m[22mN[0m
               Locally typical sampling, parameter p.

               [1m-   [22m1.0 = disabled

               Default: 1.0

       [1m--repeat‚Äêlast‚Äên [4m[22mN[0m
               Last n tokens to consider for penalize.

               [1m-   [22m0 = disabled
               [1m-   [22m‚Äê1 = ctx_size

               Default: 64

       [1m--repeat‚Äêpenalty [4m[22mN[0m
               Penalize repeat sequence of tokens.

               [1m-   [22m1.0 = disabled

               Default: 1.1

       [1m--presence‚Äêpenalty [4m[22mN[0m
               Repeat alpha presence penalty.

               [1m-   [22m0.0 = disabled

               Default: 0.0

       [1m--frequency‚Äêpenalty [4m[22mN[0m
               Repeat alpha frequency penalty.

               [1m-   [22m0.0 = disabled

               Default: 0.0

       [1m--mirostat [4m[22mN[0m
               Use  Mirostat  sampling.  Top K, Nucleus, Tail Free and Locally
               Typical samplers are ignored if used..

               [1m-   [22m0 = disabled
               [1m-   [22m1 = Mirostat
               [1m-   [22m2 = Mirostat 2.0

               Default: 0

       [1m--mirostat‚Äêlr [4m[22mN[0m
               Mirostat learning rate, parameter eta.

               Default: 0.1

       [1m--mirostat‚Äêent [4m[22mN[0m
               Mirostat target entropy, parameter tau.

               Default: 5.0

       [1m-l [4m[22mTOKEN_ID(+/‚Äê)BIAS[24m, [1m--logit‚Äêbias [4m[22mTOKEN_ID(+/‚Äê)BIAS[0m
               Modifies the likelihood of token appearing in  the  completion,
               i.e.   [1m--logit‚Äêbias  [4m[22m15043+1[24m  to  increase  likelihood of token
               [4m‚Äô[24m [4mHello‚Äô[24m, or [1m--logit‚Äêbias [4m[22m15043‚Äê1[24m to decrease likelihood of to‚Äê
               ken [4m‚Äô[24m [4mHello‚Äô[24m.

       [1m-md [4m[22mFNAME[24m, [1m--model‚Äêdraft [4m[22mFNAME[0m
               Draft model for speculative decoding.

               Default: [4mmodels/7B/ggml‚Äêmodel‚Äêf16.gguf[0m

       [1m--cfg‚Äênegative‚Äêprompt [4m[22mPROMPT[0m
               Negative prompt to use for guidance..

               Default: empty

       [1m--cfg‚Äênegative‚Äêprompt‚Äêfile [4m[22mFNAME[0m
               Negative prompt file to use for guidance.

               Default: empty

       [1m--cfg‚Äêscale [4m[22mN[0m
               Strength of guidance.

               [1m-   [22m1.0 = disable

               Default: 1.0

       [1m--rope‚Äêscaling [4m[22m{none,linear,yarn}[0m
               RoPE frequency scaling method, defaults to linear unless speci‚Äê
               fied by the model

       [1m--rope‚Äêscale [4m[22mN[0m
               RoPE context scaling factor, expands context by a factor  of  [4mN[0m
               where  [4mN[24m  is  the  linear scaling factor used by the fine‚Äêtuned
               model. Some fine‚Äêtuned models have extended the context  length
               by scaling RoPE. For example, if the original pre‚Äêtrained model
               have  a  context  length (max sequence length) of 4096 (4k) and
               the fine‚Äêtuned model have 32k. That is a scaling factor  of  8,
               and  should work by setting the above [1m--ctx‚Äêsize [22mto 32768 (32k)
               and [1m--rope‚Äêscale [22mto 8.

       [1m--rope‚Äêfreq‚Äêbase [4m[22mN[0m
               RoPE base frequency, used by NTK‚Äêaware scaling.

               Default: loaded from model

       [1m--rope‚Äêfreq‚Äêscale [4m[22mN[0m
               RoPE frequency scaling factor, expands context by a  factor  of
               1/N

       [1m--yarn‚Äêorig‚Äêctx [4m[22mN[0m
               YaRN: original context size of model.

               Default: 0 = model training context size

       [1m--yarn‚Äêext‚Äêfactor [4m[22mN[0m
               YaRN: extrapolation mix factor.

               [1m-   [22m0.0 = full interpolation

               Default: 1.0

       [1m--yarn‚Äêattn‚Äêfactor [4m[22mN[0m
               YaRN: scale sqrt(t) or attention magnitude.

               Default: 1.0

       [1m--yarn‚Äêbeta‚Äêslow [4m[22mN[0m
               YaRN: high correction dim or alpha.

               Default: 1.0

       [1m--yarn‚Äêbeta‚Äêfast [4m[22mN[0m
               YaRN: low correction dim or beta.

               Default: 32.0

       [1m--ignore‚Äêeos[0m
               Ignore  end  of  stream  token and continue generating (implies
               [1m--logit‚Äêbias [4m[22m2‚Äêinf[24m)

       [1m--no‚Äêpenalize‚Äênl[0m
               Do not penalize newline token.

       [1m--temp [4m[22mN[0m
               Temperature.

               Default: 0.8

       [1m--logits‚Äêall[0m
               Return logits for all tokens in the batch.

               Default: disabled

       [1m--hellaswag[0m
               Compute HellaSwag score over random tasks  from  datafile  sup‚Äê
               plied with ‚Äêf

       [1m--hellaswag‚Äêtasks [4m[22mN[0m
               Number of tasks to use when computing the HellaSwag score.

               Default: 400

       [1m--keep [4m[22mN[0m
               This  flag  allows users to retain the original prompt when the
               model runs out of context, ensuring a connection to the initial
               instruction or conversation topic is maintained, where [4mN[24m is the
               number of tokens from the initial prompt  to  retain  when  the
               model resets its internal context.

               [1m-   [22m0 = no tokens are kept from initial prompt
               [1m-   [22m‚Äê1 = retain all tokens from initial prompt

               Default: 0

       [1m--draft [4m[22mN[0m
               Number of tokens to draft for speculative decoding.

               Default: 16

       [1m--chunks [4m[22mN[0m
               Max number of chunks to process.

               [1m-   [22m‚Äê1 = all

               Default: ‚Äê1

       [1m-ns [4m[22mN[24m, [1m--sequences [4m[22mN[0m
               Number of sequences to decode.

               Default: 1

       [1m-pa [4m[22mN[24m, [1m--p‚Äêaccept [4m[22mN[0m
               speculative decoding accept probability.

               Default: 0.5

       [1m-ps [4m[22mN[24m, [1m--p‚Äêsplit [4m[22mN[0m
               Speculative decoding split probability.

               Default: 0.1

       [1m--mlock[0m
               Force  system to keep model in RAM rather than swapping or com‚Äê
               pressing.

       [1m--no‚Äêmmap[0m
               Do not memory‚Äêmap model (slower load but may reduce pageouts if
               not using mlock).

       [1m--numa  [22mAttempt optimizations that help on some  NUMA  systems  if  run
               without  this  previously, it is recommended to drop the system
               page       cache       before       using       this.       See
               https://github.com/ggerganov/llama.cpp/issues/1437.

       [1m--recompile[0m
               Force GPU support to be recompiled at runtime if possible.

       [1m--nocompile[0m
               Never compile GPU support at runtime.

               If  the appropriate DSO file already exists under [4m~/.llamafile/[0m
               then it‚Äôll be linked as‚Äêis without question. If a prebuilt  DSO
               is  present  in the PKZIP content of the executable, then it‚Äôll
               be extracted and linked if possible. Otherwise, [1mllamafile  [22mwill
               skip any attempt to compile GPU support and simply fall back to
               using CPU inference.

       [1m--gpu [4m[22mGPU[0m
               Specifies which brand of GPU should be used. Valid choices are:

               [1m-   [4m[22mAUTO[24m:  Use  any GPU if possible, otherwise fall back to CPU
                   inference (default)

               [1m-   [4m[22mAPPLE[24m: Use Apple Metal GPU. This is only available on MacOS
                   ARM64. If Metal could not be used for any  reason,  then  a
                   fatal error will be raised.

               [1m-   [4m[22mAMD[24m: Use AMD GPUs. The AMD HIP ROCm SDK should be installed
                   in  which  case we assume the HIP_PATH environment variable
                   has been defined. The set of gfx microarchitectures  needed
                   to  run  on  the  host  machine is determined automatically
                   based on the output of the  hipInfo  command.  On  Windows,
                   [1mllamafile  [22mrelease binaries are distributed with a tinyBLAS
                   DLL so it‚Äôll work out of the box without requiring the  HIP
                   SDK  to  be  installed.  However,  tinyBLAS  is slower than
                   rocBLAS for batch and image processing, so it‚Äôs recommended
                   that the SDK be installed anyway. If an AMD GPU  could  not
                   be used for any reason, then a fatal error will be raised.

               [1m-   [4m[22mNVIDIA[24m: Use NVIDIA GPUs. If an NVIDIA GPU could not be used
                   for  any  reason, a fatal error will be raised. On Windows,
                   NVIDIA GPU support will use our tinyBLAS library, since  it
                   works  on  stock  Windows  installs. However, tinyBLAS goes
                   slower for batch and image processing. It‚Äôs possible to use
                   NVIDIA‚Äôs closed‚Äêsource cuBLAS library instead. To do  that,
                   both  MSVC  and CUDA need to be installed and the [1mllamafile[0m
                   command should be run once from the x64 MSVC command prompt
                   with the [1m--recompile [22mflag passed.  The  GGML  library  will
                   then  be compiled and saved to [4m~/.llamafile/[24m so the special
                   process only needs to happen a single time.

               [1m-   [4m[22mDISABLE[24m: Never use GPU and instead use CPU inference.  This
                   setting is implied by [1m-ngl [4m[22m0[24m.

       [1m-ngl [4m[22mN[24m, [1m--n‚Äêgpu‚Äêlayers [4m[22mN[0m
               Number of layers to store in VRAM.

       [1m-ngld [4m[22mN[24m, [1m--n‚Äêgpu‚Äêlayers‚Äêdraft [4m[22mN[0m
               Number of layers to store in VRAM for the draft model.

       [1m-sm [4m[22mSPLIT_MODE[24m, [1m--split‚Äêmode [4m[22mSPLIT_MODE[0m
               How to split the model across multiple GPUs, one of:
               [1m-   [22mnone: use one GPU only
               [1m-   [22mlayer (default): split layers and KV across GPUs
               [1m-   [22mrow: split rows across GPUs

       [1m-ts [4m[22mSPLIT[24m, [1m--tensor‚Äêsplit [4m[22mSPLIT[0m
               When using multiple GPUs this option controls how large tensors
               should  be  split  across all GPUs.  [4mSPLIT[24m is a comma‚Äêseparated
               list of non‚Äênegative values that assigns the proportion of data
               that each GPU should get in order. For example, "3,2" will  as‚Äê
               sign  60% of the data to GPU 0 and 40% to GPU 1. By default the
               data is split in proportion to VRAM but this may not be optimal
               for performance. Requires cuBLAS.  How to split tensors  across
               multiple GPUs, comma‚Äêseparated list of proportions, e.g. 3,1

       [1m-mg [4m[22mi[24m, [1m--main‚Äêgpu [4m[22mi[0m
               The GPU to use for scratch and small tensors.

       [1m-nommq[22m, [1m--no‚Äêmul‚Äêmat‚Äêq[0m
               Use cuBLAS instead of custom mul_mat_q CUDA kernels. Not recom‚Äê
               mended since this is both slower and uses more VRAM.

       [1m--verbose‚Äêprompt[0m
               Print prompt before generation.

       [1m--simple‚Äêio[0m
               Use  basic IO for better compatibility in subprocesses and lim‚Äê
               ited consoles.

       [1m--lora [4m[22mFNAME[0m
               Apply LoRA adapter (implies [1m--no‚Äêmmap[22m)

       [1m--lora‚Äêscaled [4m[22mFNAME[24m [4mS[0m
               Apply  LoRA  adapter  with  user  defined  scaling  S  (implies
               [1m--no‚Äêmmap[22m)

       [1m--lora‚Äêbase [4m[22mFNAME[0m
               Optional  model to use as a base for the layers modified by the
               LoRA adapter

       [1m--unsecure[0m
               Disables pledge() sandboxing on Linux and OpenBSD.

       [1m--samplers[0m
               Samplers that will be used for generation in the  order,  sepa‚Äê
               rated    by    semicolon,    for    example:    top_k;tfs;typi‚Äê
               cal;top_p;min_p;temp

       [1m--samplers‚Äêseq[0m
               Simplified sequence for samplers that will be used.

       [1m-cml[22m, [1m--chatml[0m
               Run in chatml mode (use with ChatML‚Äêcompatible models)

       [1m-dkvc[22m, [1m--dump‚Äêkv‚Äêcache[0m
               Verbose print of the KV cache.

       [1m-nkvo[22m, [1m--no‚Äêkv‚Äêoffload[0m
               Disable KV offload.

       [1m-ctk [4m[22mTYPE[24m, [1m--cache‚Äêtype‚Äêk [4m[22mTYPE[0m
               KV cache data type for K.

       [1m-ctv [4m[22mTYPE[24m, [1m--cache‚Äêtype‚Äêv [4m[22mTYPE[0m
               KV cache data type for V.

       [1m-gan [4m[22mN[24m, [1m--grp‚Äêattn‚Äên [4m[22mN[0m
               Group‚Äêattention factor.

               Default: 1

       [1m-gaw [4m[22mN[24m, [1m--grp‚Äêattn‚Äêw [4m[22mN[0m
               Group‚Äêattention width.

               Default: 512

       [1m-bf [4m[22mFNAME[24m, [1m--binary‚Äêfile [4m[22mFNAME[0m
               Binary file containing multiple choice tasks.

       [1m--winogrande[0m
               Compute Winogrande score over random tasks from  datafile  sup‚Äê
               plied by the [1m-f [22mflag.

       [1m--winogrande‚Äêtasks [4m[22mN[0m
               Number of tasks to use when computing the Winogrande score.

               Default: 0

       [1m--multiple‚Äêchoice[0m
               Compute  multiple  choice score over random tasks from datafile
               supplied by the [1m-f [22mflag.

       [1m--multiple‚Äêchoice‚Äêtasks [4m[22mN[0m
               Number of tasks to  use  when  computing  the  multiple  choice
               score.

               Default: 0

       [1m--kl‚Äêdivergence[0m
               Computes    KL‚Äêdivergence    to   logits   provided   via   the
               [1m--kl‚Äêdivergence‚Äêbase [22mflag.

       [1m--save‚Äêall‚Äêlogits [4m[22mFNAME[24m, [1m--kl‚Äêdivergence‚Äêbase [4m[22mFNAME[0m
               Save logits to filename.

       [1m-ptc [4m[22mN[24m, [1m--print‚Äêtoken‚Äêcount [4m[22mN[0m
               Print token count every [4mN[24m tokens.

               Default: ‚Äê1

       [1m--pooling [4m[22mKIND[0m
               Specifies pooling type for embeddings. This may be one of:

               [1m-   [22mnone
               [1m-   [22mmean
               [1m-   [22mcls

               The model default is used if unspecified.

[1mCLI OPTIONS[0m
       The following options may be specified when  [1mllamafile  [22mis  running  in
       [1m--cli [22mmode.

       [1m-e[22m, [1m--escape[0m
               Process prompt escapes sequences (\n, \r, \t, \¬¥, \", \\)

       [1m-p [4m[22mSTRING[24m, [1m--prompt [4m[22mSTRING[0m
               Prompt  to  start  text generation. Your LLM works by auto‚Äêcom‚Äê
               pleting this text. For example:

                     [1mllamafile ‚Äêm model.gguf ‚Äêp "four score and"[0m

               Stands a pretty good chance of  printing  Lincoln‚Äôs  Gettysburg
               Address.   Prompts can take on a structured format too. Depend‚Äê
               ing on how your model was trained, it may specify in  its  docs
               an instruction notation. With some models that might be:

                     [1mllamafile ‚Äêp "[INST]Summarize this: $(cat file)[/INST]"[0m

               In most cases, simply colons and newlines will work too:

                     [1mllamafile ‚Äêe ‚Äêp "User: What is best in life?\nAssistant:"[0m

       [1m-f [4m[22mFNAME[24m, [1m--file [4m[22mFNAME[0m
               Prompt file to start generation.

       [1m--grammar [4m[22mGRAMMAR[0m
               BNF‚Äêlike grammar to constrain which tokens may be selected when
               generating text. For example, the grammar:

                     [1mroot ::= "yes" | "no"[0m

               will  force  the  LLM  to only output yes or no before exiting.
               This is useful for shell scripts when  the  [1m--no‚Äêdisplay‚Äêprompt[0m
               flag is also supplied.

       [1m--grammar‚Äêfile [4m[22mFNAME[0m
               File to read grammar from.

       [1m--fast  [22mPut  llamafile  into  fast  math mode. This disables algorithms
               that reduce floating point rounding, e.g. Kahan summation,  and
               certain functions like expf() will be vectorized but handle un‚Äê
               derflows  less  gracefully.  It‚Äôs unspecified whether llamafile
               runs in fast or precise math mode when neither flag  is  speci‚Äê
               fied.

       [1m--precise[0m
               Put  llamafile  into precise math mode. This enables algorithms
               that reduce floating point rounding, e.g. Kahan summation,  and
               certain  functions  like  expf()  will always handle subnormals
               correctly. It‚Äôs unspecified whether llamafile runs in  fast  or
               precise math mode when neither flag is specified.

       [1m--trap  [22mPut  llamafile into math trapping mode. When floating point ex‚Äê
               ceptions occur, such as NaNs, overflow,  and  divide  by  zero,
               llamafile  will  print  a  warning to the console. This warning
               will include a C++ backtrace the first  time  an  exception  is
               trapped.  The  op graph will also be dumped to a file, and lla‚Äê
               mafile will report the specific  op  where  the  exception  oc‚Äê
               curred.  This  is useful for troubleshooting when reporting is‚Äê
               sues.  USing this feature will disable sandboxing.  Math  trap‚Äê
               ping  is  only possible if your CPU supports it. That is gener‚Äê
               ally the case on AMD64, however it‚Äôs less common on ARM64.

       [1m--prompt‚Äêcache [4m[22mFNAME[0m
               File to cache prompt state for faster startup.

               Default: none

       [1m-fa [4m[22mFNAME[24m, [1m--flash‚Äêattn[0m
               Enable Flash Attention. This is a  mathematical  shortcut  that
               can  speed  up  inference  for  certain models. This feature is
               still under active development.

       [1m--prompt‚Äêcache‚Äêall[0m
               If specified, saves user input  and  generations  to  cache  as
               well. Not supported with [1m--interactive [22mor other interactive op‚Äê
               tions.

       [1m--prompt‚Äêcache‚Äêro[0m
               If specified, uses the prompt cache but does not update it.

       [1m--random‚Äêprompt[0m
               Start with a randomized prompt.

       [1m--image [4m[22mIMAGE_FILE[0m
               Path to an image file. This should be used with multimodal mod‚Äê
               els.   Alternatively,  it‚Äôs possible to embed an image directly
               into the prompt instead; in which case, it must be  base64  en‚Äê
               coded  into  an HTML img tag URL with the image/jpeg MIME type.
               See also the [1m--mmproj [22mflag for supplying the vision model.

       [1m-i[22m, [1m--interactive[0m
               Run the program in interactive mode, allowing users  to  engage
               in  real‚Äêtime conversations or provide specific instructions to
               the model.

       [1m--interactive‚Äêfirst[0m
               Run the program in interactive mode and  immediately  wait  for
               user input before starting the text generation.

       [1m-ins[22m, [1m--instruct[0m
               Run  the program in instruction mode, which is specifically de‚Äê
               signed to work with Alpaca  models  that  excel  in  completing
               tasks based on user instructions.

               Technical details: The user‚Äôs input is internally prefixed with
               the  reverse prompt (or "### Instruction:" as the default), and
               followed by "### Response:" (except if you  just  press  Return
               without any input, to keep generating a longer response).

               By  understanding  and utilizing these interaction options, you
               can create engaging and dynamic experiences with the LLaMA mod‚Äê
               els, tailoring the text generation  process  to  your  specific
               needs.

       [1m-r [4m[22mPROMPT[24m, [1m--reverse‚Äêprompt [4m[22mPROMPT[0m
               Specify  one  or multiple reverse prompts to pause text genera‚Äê
               tion and switch to interactive mode. For  example,  [1m-r  [4m[22m"User:"[0m
               can  be  used  to jump back into the conversation whenever it‚Äôs
               the user‚Äôs turn to speak. This helps create a more  interactive
               and  conversational  experience.  However,  the  reverse prompt
               doesn‚Äôt work when it ends with a space. To overcome this  limi‚Äê
               tation,  you can use the [1m--in‚Äêprefix [22mflag to add a space or any
               other characters after the reverse prompt.

       [1m--color[0m
               Enable colorized output to differentiate visually  distinguish‚Äê
               ing between prompts, user input, and generated text.

       [1m--no‚Äêdisplay‚Äêprompt[22m, [1m--silent‚Äêprompt[0m
               Don‚Äôt echo the prompt itself to standard output.

       [1m--keep [4m[22mN[0m
               Specifies number of tokens to keep from the initial prompt. The
               default is ‚Äê1 which means all tokens.

       [1m--multiline‚Äêinput[0m
               Allows you to write or paste multiple lines without ending each
               in ‚Äô\‚Äô.

       [1m--cont‚Äêbatching[0m
               Enables  continuous  batching,  a.k.a. dynamic batching.  is ‚Äê1
               which means all tokens.

       [1m--embedding[0m
               In CLI mode, the embedding flag may be use to print  embeddings
               to  standard output. By default, embeddings are computed over a
               whole prompt. However the [1m--multiline [22mflag may  be  passed,  to
               have a separate embeddings array computed for each line of text
               in  the prompt. In multiline mode, each embedding array will be
               printed on its own line to standard  output,  where  individual
               floats  are  separated  by space. If both the [1m--multiline‚Äêinput[0m
               and [1m--interactive [22mflags are passed, then a pretty‚Äêprinted  sum‚Äê
               mary  of  embeddings along with a cosine similarity matrix will
               be printed to the terminal.

[1mSERVER OPTIONS[0m
       The following options may be specified when  [1mllamafile  [22mis  running  in
       [1m--server [22mmode.

       [1m--port [4m[22mPORT[0m
               Port to listen

               Default: 8080

       [1m--host [4m[22mIPADDR[0m
               IP address to listen.

               Default: 127.0.0.1

       [1m-to [4m[22mN[24m, [1m--timeout [4m[22mN[0m
               Server read/write timeout in seconds.

               Default: 600

       [1m-np [4m[22mN[24m, [1m--parallel [4m[22mN[0m
               Number of slots for process requests.

               Default: 1

       [1m-cb[22m, [1m--cont‚Äêbatching[0m
               Enable continuous batching (a.k.a dynamic batching).

               Default: disabled

       [1m-spf [4m[22mFNAME[24m, [1m--system‚Äêprompt‚Äêfile [4m[22mFNAME[0m
               Set  a  file  to  load  a  system prompt (initial prompt of all
               slots), this is useful for chat applications.

       [1m-a [4m[22mALIAS[24m, [1m--alias [4m[22mALIAS[0m
               Set an alias for the model. This will be  added  as  the  [4mmodel[0m
               field in completion responses.

       [1m--path [4m[22mPUBLIC_PATH[0m
               Path from which to serve static files.

               Default: [4m/zip/llama.cpp/server/public[0m

       [1m--nobrowser[0m
               Do not attempt to open a web browser tab at startup.

       [1m-gan [4m[22mN[24m, [1m--grp‚Äêattn‚Äên [4m[22mN[0m
               Set  the  group attention factor to extend context size through
               self‚Äêextend. The default value is [4m1[24m which means disabled.  This
               flag is used together with [1m--grp‚Äêattn‚Äêw[22m.

       [1m-gaw [4m[22mN[24m, [1m--grp‚Äêattn‚Äêw [4m[22mN[0m
               Set  the  group  attention width to extend context size through
               self‚Äêextend. The default value is [4m512[24m.  This flag is  used  to‚Äê
               gether with [1m--grp‚Äêattn‚Äên[22m.

[1mLOG OPTIONS[0m
       The following log options are available:

       [1m-ld [4m[22mLOGDIR[24m, [1m--logdir [4m[22mLOGDIR[0m
               Path under which to save YAML logs (no logging if unset)

       [1m--log‚Äêtest[0m
               Run simple logging test

       [1m--log‚Äêdisable[0m
               Disable trace logs

       [1m--log‚Äêenable[0m
               Enable trace logs

       [1m--log‚Äêfile[0m
               Specify a log filename (without extension)

       [1m--log‚Äênew[0m
               Create  a  separate  new  log file on start. Each log file will
               have unique name: [4m<name>.<ID>.log[0m

       [1m--log‚Äêappend[0m
               Don‚Äôt truncate the old log file.

[1mEXAMPLES[0m
       Here‚Äôs an example of how to run llama.cpp‚Äôs built‚Äêin HTTP server.  This
       example   uses   LLaVA  v1.5‚Äê7B,  a  multimodal  LLM  that  works  with
       llama.cpp‚Äôs recently‚Äêadded support for image inputs.

             llamafile \
               ‚Äêm llava‚Äêv1.5‚Äê7b‚ÄêQ8_0.gguf \
               ‚Äê‚Äêmmproj llava‚Äêv1.5‚Äê7b‚Äêmmproj‚ÄêQ8_0.gguf \
               ‚Äê‚Äêhost 0.0.0.0

       Here‚Äôs an example of how to generate code for a libc function using the
       llama.cpp  command  line  interface,  utilizing  WizardCoder‚ÄêPython‚Äê13B
       weights:

             llamafile \
               ‚Äêm wizardcoder‚Äêpython‚Äê13b‚Äêv1.0.Q8_0.gguf ‚Äê‚Äêtemp 0 ‚Äêr ‚Äô}\n‚Äô ‚Äêr ‚Äô```\n‚Äô \
               ‚Äêe ‚Äêp ‚Äô```c\nvoid *memcpy(void *dst, const void *src, size_t size) {\n‚Äô

       Here‚Äôs  a  similar  example  that  instead utilizes Mistral‚Äê7B‚ÄêInstruct
       weights for prose composition:

             llamafile \
               ‚Äêm mistral‚Äê7b‚Äêinstruct‚Äêv0.2.Q5_K_M.gguf \
               ‚Äêp ‚Äô[INST]Write a story about llamas[/INST]‚Äô

       Here‚Äôs an example of how llamafile can be used as an interactive  chat‚Äê
       bot that lets you query knowledge contained in training data:

             llamafile ‚Äêm llama‚Äê65b‚ÄêQ5_K.gguf ‚Äêp ‚Äô
             The following is a conversation between a Researcher and their helpful AI
             assistant Digital Athena which is a large language model trained on the
             sum of human knowledge.
             Researcher: Good morning.
             Digital Athena: How can I help you today?
             Researcher:‚Äô ‚Äê‚Äêinteractive ‚Äê‚Äêcolor ‚Äê‚Äêbatch_size 1024 ‚Äê‚Äêctx_size 4096 \
             ‚Äê‚Äêkeep ‚Äê1 ‚Äê‚Äêtemp 0 ‚Äê‚Äêmirostat 2 ‚Äê‚Äêin‚Äêprefix ‚Äô ‚Äô ‚Äê‚Äêinteractive‚Äêfirst \
             ‚Äê‚Äêin‚Äêsuffix ‚ÄôDigital Athena:‚Äô ‚Äê‚Äêreverse‚Äêprompt ‚ÄôResearcher:‚Äô

       Here‚Äôs an example of how you can use llamafile to summarize HTML URLs:

             (
               echo ‚Äô[INST]Summarize the following text:‚Äô
               links ‚Äêcodepage utf‚Äê8 \
                     ‚Äêforce‚Äêhtml \
                     ‚Äêwidth 500 \
                     ‚Äêdump https://www.poetryfoundation.org/poems/48860/the‚Äêraven |
                 sed ‚Äôs/   */ /g‚Äô
               echo ‚Äô[/INST]‚Äô
             ) | llamafile \
                   ‚Äêm mistral‚Äê7b‚Äêinstruct‚Äêv0.2.Q5_K_M.gguf \
                   ‚Äêf /dev/stdin \
                   ‚Äêc 0 \
                   ‚Äê‚Äêtemp 0 \
                   ‚Äên 500 \
                   ‚Äê‚Äêno‚Äêdisplay‚Äêprompt 2>/dev/null

       Here‚Äôs how you can use llamafile to describe a jpg/png/gif/bmp image:

             llamafile ‚Äê‚Äêtemp 0 \
               ‚Äê‚Äêimage lemurs.jpg \
               ‚Äêm llava‚Äêv1.5‚Äê7b‚ÄêQ4_K.gguf \
               ‚Äê‚Äêmmproj llava‚Äêv1.5‚Äê7b‚Äêmmproj‚ÄêQ4_0.gguf \
               ‚Äêe ‚Äêp ‚Äô### User: What do you see?\n### Assistant: ‚Äô \
               ‚Äê‚Äêno‚Äêdisplay‚Äêprompt 2>/dev/null

       If  you  wanted  to  write a script to rename all your image files, you
       could use the following command to generate a safe filename:

             llamafile ‚Äê‚Äêtemp 0 \
                 ‚Äê‚Äêimage ~/Pictures/lemurs.jpg \
                 ‚Äêm llava‚Äêv1.5‚Äê7b‚ÄêQ4_K.gguf \
                 ‚Äê‚Äêmmproj llava‚Äêv1.5‚Äê7b‚Äêmmproj‚ÄêQ4_0.gguf \
                 ‚Äê‚Äêgrammar ‚Äôroot ::= [a‚Äêz]+ (" " [a‚Äêz]+)+‚Äô \
                 ‚Äêe ‚Äêp ‚Äô### User: The image has...\n### Assistant: ‚Äô \
                 ‚Äê‚Äêno‚Äêdisplay‚Äêprompt 2>/dev/null |
               sed ‚Äêe‚Äôs/ /_/g‚Äô ‚Äêe‚Äôs/$/.jpg/‚Äô
             three_baby_lemurs_on_the_back_of_an_adult_lemur.jpg

       Here‚Äôs an example of how to make an API request to the OpenAI API  com‚Äê
       patible  completions  endpoint  when  your  [1mllamafile [22mis running in the
       background in [1m--server [22mmode.

             curl ‚Äês http://localhost:8080/v1/chat/completions \
                  ‚ÄêH "Content‚ÄêType: application/json" ‚Äêd ‚Äô{
               "model": "gpt‚Äê3.5‚Äêturbo",
               "stream": true,
               "messages": [
                 {
                   "role": "system",
                   "content": "You are a poetic assistant."
                 },
                 {
                   "role": "user",
                   "content": "Compose a poem that explains FORTRAN."
                 }
               ]
             }‚Äô | python3 ‚Äêc ‚Äô
             import json
             import sys
             json.dump(json.load(sys.stdin), sys.stdout, indent=2)
             print()

[1mPROTIP[0m
       The [1m-ngl [4m[22m35[24m flag needs to be passed in order to use GPUs made by NVIDIA
       and AMD.  It‚Äôs not enabled by default since it sometimes  needs  to  be
       tuned  based on the system hardware and model architecture, in order to
       achieve optimal performance, and avoid compromising a shared display.

[1mSEE ALSO[0m
       [4mllamafile‚Äêquantize[24m(1),   [4mllamafile‚Äêperplexity[24m(1),    [4mllava‚Äêquantize[24m(1),
       [4mzipalign[24m(1), [4munzip[24m(1)

Mozilla Ocho                    January 1, 2024                   [4mLLAMAFILE[24m(1)
