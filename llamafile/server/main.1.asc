[4mLLAMAFILER[24m(1)               General Commands Manual              [4mLLAMAFILER[24m(1)

[1mNAME[0m
       llamafiler ‚Äî fast reliable large language model server

[1mSYNOPSIS[0m
       [1mllamafiler -m [4m[22mmodel.gguf[24m [flags...]

[1mDESCRIPTION[0m
       [1mllamafiler  [22mllamafiler  is  an  HTTP  server  for Large Language Models
       (LLMs). It includes a web GUI for both chatbot and text completion.  It
       can  be your OpenAI API compatible embeddings / completions / chat com‚Äê
       pletions server. It's able to more intelligently recycle  context  win‚Äê
       dows across multiple slots serving multiple clients.

[1mOPTIONS[0m
       The following options are available:

       [1m--version[0m
               Print version and exit.

       [1m-h[22m, [1m--help[0m
               Show help message and exit.

       [1m-m [4m[22mFNAME[24m, [1m--model [4m[22mFNAME[0m
               Path  of  GGUF  model weights. Each server process is currently
               limited to serving only one model. If you need to host multiple
               models, then it's recommended that you run  multiple  instances
               of llamafiler behind a reverse proxy such as NGINX or Redbean.

       [1m-mm [4m[22mFNAME[24m, [1m--mmproj [4m[22mFNAME[0m
               Path of vision model weights.

       [1m--db [4m[22mFILE[0m
               Specifies path of sqlite3 database.

               The default is [4m~/.llamafile/llamafile.sqlite3[0m

       [1m-ngl [4m[22mN[24m, [1m--gpu-layers [4m[22mN[24m, [1m--n-gpu-layers [4m[22mN[0m
               Specifies number of layers to offload to GPU.

               This  flag  must  be passed in order to use GPU on systems with
               NVIDIA or AMD GPUs. If you're confident that  you  have  enough
               VRAM,  then  you  can  pass [1m-ngl [4m[22m999[24m to enable full offloading,
               since this number is automatically downtuned  to  however  many
               number  of  layers  the model has. If VRAM is limited, then the
               [1m--verbose [22mflag may be passed to learn how many layers the model
               has, e.g. 35, which can then be down-tuned  until  the  out  of
               memory error goes away.

               On  Apple Silicon systems with Metal, GPU offloading is enabled
               by default.  Since  these  GPUs  use  unified  memory,  they're
               treated  as  having  a  single  layer;  therefore, using values
               higher than 1 will be treated as 1. You can pass [1m-ngl [4m[22m0[24m to dis‚Äê
               able GPU offloading and run in CPU mode on Apple Metal systems.

       [1m-l [4m[22mHOSTPORT[24m, [1m--listen [4m[22mHOSTPORT[0m
               Specifies the local [HOST:]PORT on which the HTTP server should
               listen.  By default this is 0.0.0.0:8080 which means llamafiler
               will bind to port 8080 on every locally available IPv4  network
               interface. This option may currently only be specified once.

       [1m-c [4m[22mTOKENS[24m, [1m--ctx-size [4m[22mTOKENS[0m
               Specifies  context  size.  This specifies how long a completion
               can get before it runs out of space. It defaults  to  8k  which
               means  8192 tokens.  Many models support a larger context size,
               like 128k, but that'll need much more RAM or VRAM per slot.  If
               this  value  is  larger  than  the  trained context size of the
               model, it'll be tuned down to the maximum. If this value  is  0
               or negative, the maximum number of tokens will be used.

       [1m-s [4m[22mCOUNT[24m, [1m--slots [4m[22mCOUNT[0m
               Specifies how many slots to maintain. This defaults to 1. Slots
               are  used  by  chat  completions  requests. When such a request
               comes in, the client needs to take control of a slot. When  the
               completion  is  finished,  the slot is relinquished back to the
               server. HTTP clients will wait for a slot to be relinquished if
               none are available. Tuning this parameter to nicely fit  avail‚Äê
               able RAM or VRAM can help you manage your server resources, and
               control  how  much  completion  parallelism can happen.  Please
               note that [1m--ctx-size [22mhas a strong influence on how  many  slots
               can be created.

       [1m--decay-delay [4m[22mINT[0m
               Number  of  seconds  a context window slot needs to be inactive
               before the system starts to  strongly  consider  giving  it  to
               other clients. The default is 300 which is five minutes.

       [1m--decay-growth [4m[22mFLOAT[0m
               Sets  slot  decay  growth  factor. Context window slots are as‚Äê
               signed in a least recently used fashion, based on  the  formula
               age + e sup {growth * (age - delay)}

       [1m-p [4m[22mTEXT[24m, [1m--prompt [4m[22mTEXT[24m, [1m--system-prompt [4m[22mTEXT[0m
               Specifies  system prompt. This value is passed along to the web
               frontend.

       [1m--no-display-prompt[0m
               Hide system prompt from web user interface.

       [1m--nologo[0m
               Hide llamafile logo icon from web ui.

       [1m--url-prefix [4m[22mURLPREFIX[0m
               Specifies a URL prefix  (subdirectory)  under  which  the  HTTP
               server  will  make  the API accessible, e.g. /lamafiler. Useful
               when running llamafiler behind a reverse proxy such as NGINX or
               Redbean. By default, this is set to / (root).

       [1m--verbose[0m
               Enable logging of diagnostic information. This flag  is  useful
               for  learning more about the model and hardware. It can also be
               helpful for troubleshooting errors. We currently recommend that
               this flag be avoided in production since the  llama.cpp  logger
               may disrupt thread cancelation.

       [1m-w [4m[22mN[24m, [1m--workers [4m[22mN[0m
               Number of HTTP client handling threads.

       [1m--trust [4m[22mCIDR[0m
               Adds  a  network  to the trusted network list. This argument is
               specified in the form IPV4/MASKBITS,  e.g.  192.168.0.0/24.  By
               default, all clients are untrusted, which means they're subject
               to token bucket throttling, and additional security precautions
               that  may  cause request handling to go slightly slower. There‚Äê
               fore this flag is important to use if you  want  to  accurately
               benchmark  llamafiler,  since the server will otherwise see the
               benchmark as a DDOS and deprioritize its traffic accordingly.

       [1m--ip-header [4m[22mSTR[0m
               If this flag is passed a value, e.g. X-Forwarded-For, then  any
               trusted may send this header to your llamafile server to let it
               know  what  the true effective client IPv4 address actually is.
               After this happens the default security restrictions, e.g.  to‚Äê
               ken  bucket, will be measured and applied against that IPv4 ad‚Äê
               dress and its adjacent networks.

       [1m--token-rate [4m[22mN[0m
               Specifies how many times per second a token is dropped in  each
               bucket.   This  setting  is  used to define a limitation on how
               many TCP connects and HTTP messages each chunk of the IPv4  ad‚Äê
               dress space is permitted to send to llamafiler over a sustained
               period  of time. The default token rate is 1, which means that,
               on a long enough timeline, a class-C network will be  depriori‚Äê
               tized  if  it  sends  more than one request per second. No real
               penalty actually applies though until the server  runs  out  of
               resources, e.g. HTTP request workers.

       [1m--token-burst [4m[22mN[0m
               Specifies how many HTTP requests and TCP connects a given slice
               of  the  IPv4 address space is permitted to send within a short
               period of time, before token bucket restrictions kick  in,  and
               cause the client to be deprioritized. By default, this value is
               set  to 100. It may be tuned to any value between 1 and 127 in‚Äê
               clusive.

       [1m--token-cidr [4m[22mN[0m
               Specifies IPv4 address space granularity of token bucket  algo‚Äê
               rithm,  in  network  bits.  By default, this value is set to 24
               which means individual IPv4 addresses are viewed as being  rep‚Äê
               resentative  members  of  a class-C network, or in other words,
               each group of 256 IPv4 addresses is lumped together. If one  IP
               in  the group does something bad, then bad things happen to all
               the other IPv4 addresses in that granule. This  number  may  be
               set  to  any  integer  between 3 and 32 inclusive. Specifying a
               higher number will trade away system memory to increase network
               specificity.  For example, using 32 means that 4 billion  indi‚Äê
               vidual  token buckets will be created. By default, a background
               thread drops one token in each bucket  every  second,  so  that
               could potentially be a lot of busy work. A value of three means
               that  everyone  on  the  Internet who talks to your server will
               have to fight over only eight token buckets in total.

       [1m--unsecure[0m
               Disables sandboxing. By default, llamafiler puts  itself  in  a
               SECCOMP BPF sandbox, so that even if your server gets hacked in
               the  worst  possible  way  (some  kind  of C++ memory bug) then
               there's very little damage an attacker will be able to do. This
               works by restricting system calls using Cosmopolitan Libc's im‚Äê
               plementation of pledge() which is currently only  supported  on
               Linux  (other  OSes  will  simply be unsecured by default). The
               pledge security policy that's used by default is  "stdio  anet"
               which  means  that  only  relatively harmless system calls like
               read(), write(), and accept() are allowed once the  server  has
               finished  initializing. It's not possible for remotely executed
               code to do things like launch subprocesses, read  or  write  to
               the filesystem, or initiate a new connection to a server.

       [1m-k [4m[22mN[24m, [1m--keepalive [4m[22mN[0m
               Specifies  the TCP keepalive interval in seconds. This value is
               passed along to both TCP_KEEPIDLE and TCP_KEEPINTVL if  they're
               supported  by  the  host  operating  system.  If  this value is
               greater than 0, then the the SO_KEEPALIVE and  TCP_NODELAY  op‚Äê
               tions  are enabled on network sockets, if supported by the host
               operating system. The default keepalive is 5.

       [1m--http-obuf-size [4m[22mN[0m
               Size of HTTP output buffer size, in bytes. Default is 1048576.

       [1m--http-ibuf-size [4m[22mN[0m
               Size of HTTP input buffer size, in bytes. Default is 1048576.

       [1m--chat-template [4m[22mNAME[0m
               Specifies or overrides chat template for model.

               Normally the GGUF metadata tokenizer.chat_template will specify
               this value for instruct models. This flag may be used to either
               override the chat template, or specify one when the GGUF  meta‚Äê
               data  field  is  absent, which effectively forces the web ui to
               enable chatbot mode.

               Supported chat template names are: chatml, llama2, llama3, mis‚Äê
               tral (alias for llama2), phi3, zephyr, monarch,  gemma,  gemma2
               (alias   for  gemma),  orion,  openchat,  vicuna,  vicuna-orca,
               deepseek, command-r, chatglm3, chatglm4, minicpm, deepseek2, or
               exaone3.

               It is also possible to pass the jinja2 template itself to  this
               argument.  Since llamafiler doesn't currently support jinja2, a
               heuristic  will  be  used to guess which of the above templates
               the template represents.

       [1m--completion-mode[0m
               Forces web ui to operate in completion mode, rather  than  chat
               mode.   Normally  the web ui chooses its mode based on the GGUF
               metadata. Base models normally don't define tokenizer.chat_tem‚Äê
               plate whereas instruct models do. If it's a  base  model,  then
               the web ui will automatically use completion mode only, without
               needing  to  specify  this  flag.  This flag is useful in cases
               where a prompt template is defined by the gguf, but it  is  de‚Äê
               sirable for the chat interface to be disabled.

       [1m--db-startup-sql [4m[22mCODE[0m
               Specifies  SQL code that should be executed whenever connecting
               to the SQLite database. The  default  is  the  following  code,
               which enables the write-ahead log.

                     PRAGMA journal_mode=WAL;
                     PRAGMA synchronous=NORMAL;

       [1m--reserve-tokens [4m[22mN[0m
               Percent of context window to reserve for predicted tokens. When
               the  server  runs out of context window, old chat messages will
               be forgotten until this percent of the context  is  empty.  The
               default  is  15%. If this is specified as a floating point num‚Äê
               ber, e.g. 0.15, then it'll be multiplied by 100 to get the per‚Äê
               cent.

[1mEXAMPLES[0m
       Here's an example of how you might start this server:

             [1mllamafiler -m all-MiniLM-L6-v2.F32.gguf[0m

       Here's how to send a tokenization request:

             [1mcurl -v http://127.0.0.1:8080/tokenize?prompt=hello+world[0m

       Here's how to send a embedding request:

             [1mcurl -v http://127.0.0.1:8080/embedding?content=hello+world[0m

[1mDOCUMENTATION[0m
       Read our Markdown documentation for additional help and tutorials.  See
       llamafile/server/doc/index.md in the source repository on GitHub.

[1mSEE ALSO[0m
       [4mllamafile[24m(1), [4mwhisperfile[24m(1)

Mozilla Ocho                   November 30, 2024                 [4mLLAMAFILER[24m(1)
