LLAMAFILER(1)                General Commands Manual               LLAMAFILER(1)

NNAAMMEE
     llllaammaaffiilleerr - fast reliable embedding server

SSYYNNOOPPSSIISS
     llllaammaaffiilleerr --mm _m_o_d_e_l_._g_g_u_f [flags...]

DDEESSCCRRIIPPTTIIOONN
     llllaammaaffiilleerr is a brand new HTTP server for Large Language Models (LLMs). To
     date, its development has been focused on doing fewer things really well,
     and right now that's serving embeddings. It offers you 3.4x the
     performance, stronger security, client request prioritization, request
     preemption, as well as request isolation that helps ensure software bugs
     won't cause the whole server to crash.

OOPPTTIIOONNSS
     The following options are available:

     ----vveerrssiioonn
             Print version and exit.

     --hh, ----hheellpp
             Show help message and exit.

     --mm _F_N_A_M_E, ----mmooddeell _F_N_A_M_E
             Path of GGUF model weights. Each server process is currently
             limited to serving only one model. If you need to host multiple
             models, then it's recommended that you run multiple instances of
             llamafiler behind a reverse proxy such as NGINX or Redbean.

     --mmmm _F_N_A_M_E, ----mmmmpprroojj _F_N_A_M_E
             Path of vision model weights.

     ----ddbb _F_I_L_E
             Specifies path of sqlite3 database.

             The default is _~_/_._l_l_a_m_a_f_i_l_e_/_l_l_a_m_a_f_i_l_e_._s_q_l_i_t_e_3

     --ll _H_O_S_T_P_O_R_T, ----lliisstteenn _H_O_S_T_P_O_R_T
             Specifies the local [HOST:]PORT on which the HTTP server should
             listen.  By default this is 0.0.0.0:8080 which means llamafiler
             will bind to port 8080 on every locally available IPv4 network
             interface. This option may currently only be specified once.

     --cc _T_O_K_E_N_S, ----ccttxx--ssiizzee _T_O_K_E_N_S
             Specifies context size. This specifies how long a completion can
             get before it runs out of space. It defaults to 8k which means 8192
             tokens.  Many models support a larger context size, like 128k, but
             that'll need much more RAM or VRAM per slot. If this value is
             larger than the trained context size of the model, it'll be tuned
             down to the maximum. If this value is 0 or negative, the maximum
             number of tokens will be used.

     --ss _C_O_U_N_T, ----sslloottss _C_O_U_N_T
             Specifies how many slots to maintain. This defaults to 1. Slots are
             used by chat completions requests. When such a request comes in,
             the client needs to take control of a slot. When the completion is
             finished, the slot is relinquished back to the server. HTTP clients
             will wait for a slot to be relinquished if none are available.
             Tuning this parameter to nicely fit available RAM or VRAM can help
             you manage your server resources, and control how much completion
             parallelism can happen.  Please note that ----ccttxx--ssiizzee has a strong
             influence on how many slots can be created.

     --pp _T_E_X_T, ----pprroommpptt _T_E_X_T, ----ssyysstteemm--pprroommpptt _T_E_X_T
             Specifies system prompt. This value is passed along to the web
             frontend.

     ----nnoo--ddiissppllaayy--pprroommpptt
             Hide system prompt from web user interface.

     ----nnoollooggoo
             Hide llamafile logo icon from web ui.

     ----uurrll--pprreeffiixx _U_R_L_P_R_E_F_I_X
             Specifies a URL prefix (subdirectory) under which the HTTP server
             will make the API accessible, e.g. /lamafiler. Useful when running
             llamafiler behind a reverse proxy such as NGINX or Redbean. By
             default, this is set to / (root).

     --ww _N, ----wwoorrkkeerrss _N
             Number of HTTP client handling threads.

     ----ttrruusstt _C_I_D_R
             Adds a network to the trusted network list. This argument is
             specified in the form IPV4/MASKBITS, e.g. 192.168.0.0/24. By
             default, all clients are untrusted, which means they're subject to
             token bucket throttling, and additional security precautions that
             may cause request handling to go slightly slower. Therefore this
             flag is important to use if you want to accurately benchmark
             llamafiler, since the server will otherwise see the benchmark as a
             DDOS and deprioritize its traffic accordingly.

     ----iipp--hheeaaddeerr _S_T_R
             If this flag is passed a value, e.g. X-Forwarded-For, then any
             trusted may send this header to your llamafile server to let it
             know what the true effective client IPv4 address actually is. After
             this happens the default security restrictions, e.g. token bucket,
             will be measured and applied against that IPv4 address and its
             adjacent networks.

     ----ttookkeenn--rraattee _N
             Specifies how many times per second a token is dropped in each
             bucket.  This setting is used to define a limitation on how many
             TCP connects and HTTP messages each chunk of the IPv4 address space
             is permitted to send to llamafiler over a sustained period of time.
             The default token rate is 1, which means that, on a long enough
             timeline, a class-C network will be deprioritized if it sends more
             than one request per second. No real penalty actually applies
             though until the server runs out of resources, e.g. HTTP request
             workers.

     ----ttookkeenn--bbuurrsstt _N
             Specifies how many HTTP requests and TCP connects a given slice of
             the IPv4 address space is permitted to send within a short period
             of time, before token bucket restrictions kick in, and cause the
             client to be deprioritized. By default, this value is set to 100.
             It may be tuned to any value between 1 and 127 inclusive.

     ----ttookkeenn--cciiddrr _N
             Specifies IPv4 address space granularity of token bucket algorithm,
             in network bits. By default, this value is set to 24 which means
             individual IPv4 addresses are viewed as being representative
             members of a class-C network, or in other words, each group of 256
             IPv4 addresses is lumped together. If one IP in the group does
             something bad, then bad things happen to all the other IPv4
             addresses in that granule. This number may be set to any integer
             between 3 and 32 inclusive. Specifying a higher number will trade
             away system memory to increase network specificity.  For example,
             using 32 means that 4 billion individual token buckets will be
             created. By default, a background thread drops one token in each
             bucket every second, so that could potentially be a lot of busy
             work. A value of three means that everyone on the Internet who
             talks to your server will have to fight over only eight token
             buckets in total.

     ----uunnsseeccuurree
             Disables sandboxing. By default, llamafiler puts itself in a
             SECCOMP BPF sandbox, so that even if your server gets hacked in the
             worst possible way (some kind of C++ memory bug) then there's very
             little damage an attacker will be able to do. This works by
             restricting system calls using Cosmopolitan Libc's implementation
             of pledge() which is currently only supported on Linux (other OSes
             will simply be unsecured by default). The pledge security policy
             that's used by default is "stdio anet" which means that only
             relatively harmless system calls like read(), write(), and accept()
             are allowed once the server has finished initializing. It's not
             possible for remotely executed code to do things like launch
             subprocesses, read or write to the filesystem, or initiate a new
             connection to a server.

     --kk _N, ----kkeeeeppaalliivvee _N
             Specifies the TCP keepalive interval in seconds. This value is
             passed along to both TCP_KEEPIDLE and TCP_KEEPINTVL if they're
             supported by the host operating system. If this value is greater
             than 0, then the the SO_KEEPALIVE and TCP_NODELAY options are
             enabled on network sockets, if supported by the host operating
             system. The default keepalive is 5.

     ----hhttttpp--oobbuuff--ssiizzee _N
             Size of HTTP output buffer size, in bytes. Default is 1048576.

     ----hhttttpp--iibbuuff--ssiizzee _N
             Size of HTTP input buffer size, in bytes. Default is 1048576.

     ----cchhaatt--tteemmppllaattee _N_A_M_E
             Specifies or overrides chat template for model.

             Normally the GGUF metadata tokenizer.chat_template will specify
             this value for instruct models. This flag may be used to either
             override the chat template, or specify one when the GGUF metadata
             field is absent, which effectively forces the web ui to enable
             chatbot mode.

             Supported chat template names are: chatml, llama2, llama3, mistral
             (alias for llama2), phi3, zephyr, monarch, gemma, gemma2 (alias for
             gemma), orion, openchat, vicuna, vicuna-orca, deepseek, command-r,
             chatglm3, chatglm4, minicpm, deepseek2, or exaone3.

             It is also possible to pass the jinja2 template itself to this
             argument.  Since llamafiler doesn't currently support jinja2, a
             heuristic will be used to guess which of the above templates the
             template represents.

     ----ccoommpplleettiioonn--mmooddee
             Forces web ui to operate in completion mode, rather than chat mode.
             Normally the web ui chooses its mode based on the GGUF metadata.
             Base models normally don't define tokenizer.chat_template whereas
             instruct models do. If it's a base model, then the web ui will
             automatically use completion mode only, without needing to specify
             this flag. This flag is useful in cases where a prompt template is
             defined by the gguf, but it is desirable for the chat interface to
             be disabled.

     ----ddbb--ssttaarrttuupp--ssqqll
             Specifies SQL code that should be executed whenever connecting to
             the SQLite database. The default is the following code, which
             enables the write-ahead log.

                   PRAGMA journal_mode=WAL;
                   PRAGMA synchronous=NORMAL;

     ----rreesseerrvvee--ttookkeennss
             Percent of context window to reserve for predicted tokens. When the
             server runs out of context window, old chat messages will be
             forgotten until this percent of the context is empty. The default
             is 15%. If this is specified as a floating point number, e.g. 0.15,
             then it'll be multiplied by 100 to get the percent.

EEXXAAMMPPLLEE
     Here's an example of how you might start this server:

           llamafiler -m all-MiniLM-L6-v2.F32.gguf

     Here's how to send a tokenization request:

           curl -v http://127.0.0.1:8080/tokenize?prompt=hello+world

     Here's how to send a embedding request:

           curl -v http://127.0.0.1:8080/embedding?content=hello+world

DDOOCCUUMMEENNTTAATTIIOONN
     Read our Markdown documentation for additional help and tutorials. See
     llamafile/server/doc/index.md in the source repository on GitHub.

SSEEEE AALLSSOO
     llamafile(1), whisperfile(1)

Mozilla Ocho                    November 30, 2024                   Mozilla Ocho
